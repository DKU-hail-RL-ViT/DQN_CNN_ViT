{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7dmGuNMSJ9o"
      },
      "source": [
        "# Cartpole stabilization via DQN and visual input\n",
        "\n",
        "A special thanks goes to gi`Adam Paszke <https://github.com/apaszke>`_, \n",
        "for a first implementation of the DQN algorithm with vision input in\n",
        "the Cartpole-V0 environment from OpenAI Gym.\n",
        "`Gym website <https://gym.openai.com/envs/CartPole-v0>`__.\n",
        "\n",
        "The goal of this project is to design a control system for stabilizing a\n",
        "Cart and Pole using Deep Reinforcement Learning, having only images as \n",
        "control inputs. We implement the vision-based control using the DQN algorithm\n",
        "combined with Convolutional Neural Network for Q-values approximation.\n",
        "\n",
        "The last two frames of the Cartpole are used as input, cropped and processed \n",
        "before using them in the Neural Network. In order to stabilize the training,\n",
        "we use an experience replay buffer as shown in the paper \"Playing Atari with\n",
        "Deep Reinforcement Learning:\n",
        " <https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf>__.\n",
        "\n",
        "Besides, a target network to further stabilize the training process is used.\n",
        "make the training not converge, we set a threshold for stopping training\n",
        "when we detect stable improvements: this way we learn optimal behavior\n",
        "without saturation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEE6KCibSJ9v"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from collections import deque\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-yuxvohSYPq",
        "outputId": "e2db161a-6298-4418-d8b4-5100d16c3cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "OFvGrMeySJ9w",
        "outputId": "d4964c20-32b1-4a13-a5db-06ade11b8db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdark_knight_21th\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230403_162403-legfvtcd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dark_knight_21th/cartpole-v4/runs/legfvtcd' target=\"_blank\">prime-sunset-13</a></strong> to <a href='https://wandb.ai/dark_knight_21th/cartpole-v4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dark_knight_21th/cartpole-v4' target=\"_blank\">https://wandb.ai/dark_knight_21th/cartpole-v4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dark_knight_21th/cartpole-v4/runs/legfvtcd' target=\"_blank\">https://wandb.ai/dark_knight_21th/cartpole-v4/runs/legfvtcd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dark_knight_21th/cartpole-v4/runs/legfvtcd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ff17b0b5a30>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Wandb\n",
        "import wandb\n",
        "wandb.init(project=\"cartpole-v4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bEK0cgLE1KQ",
        "outputId": "087b623b-a291-42e3-b3e3-5b8d106e2fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8h_RVp5SJ9y"
      },
      "source": [
        "Hyperparamee"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ViT"
      ],
      "metadata": {
        "id": "zFsMSTxSEmyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "#from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "import torchvision.transforms as T\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "0IG6oE4_Ey8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 2, patch_size: int = 15, emb_size: int = 765, img_size: int = [60,135]):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn(((img_size[0] // patch_size)*(img_size[1] // patch_size))+1, emb_size))\n",
        "        \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "       \n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # add position embedding\n",
        "        \n",
        "        x += self.positions\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UzKEj9ZQEeD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 765, num_heads: int = 5, dropout: float = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "        self.scaling = (self.emb_size // num_heads) ** -0.5\n",
        "\n",
        "        \n",
        "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "            \n",
        "       \n",
        "        att = F.softmax(energy * self.scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "AwJZyD_jE-dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ],
      "metadata": {
        "id": "dztC-fMxE-oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Sequential): #MLP\n",
        "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )"
      ],
      "metadata": {
        "id": "um0uRbL9FOJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size: int = 765,\n",
        "                 drop_p: float = 0.,\n",
        "                 forward_expansion: int = 4,\n",
        "                 forward_drop_p: float = 0.,\n",
        "                 ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))"
      ],
      "metadata": {
        "id": "FPDHMeCFFQ8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 3, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
      ],
      "metadata": {
        "id": "y80XjsulFT2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,     \n",
        "                in_channels: int = 2,\n",
        "                patch_size: int = 15,\n",
        "                emb_size: int = 765,\n",
        "                img_size: int = [60,135],\n",
        "                depth: int = 3,\n",
        "               \n",
        "                **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "           \n",
        "        )\n",
        "        \n",
        "summary(ViT(), (2, 60, 135), device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TVHoXN8FX2F",
        "outputId": "29c44afc-ef72-4588-bac4-c76bac00b6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 765, 4, 9]         345,015\n",
            "         Rearrange-2              [-1, 36, 765]               0\n",
            "    PatchEmbedding-3              [-1, 37, 765]               0\n",
            "         LayerNorm-4              [-1, 37, 765]           1,530\n",
            "            Linear-5             [-1, 37, 2295]       1,757,970\n",
            "           Dropout-6            [-1, 5, 37, 37]               0\n",
            "            Linear-7              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-8              [-1, 37, 765]               0\n",
            "           Dropout-9              [-1, 37, 765]               0\n",
            "      ResidualAdd-10              [-1, 37, 765]               0\n",
            "        LayerNorm-11              [-1, 37, 765]           1,530\n",
            "           Linear-12             [-1, 37, 3060]       2,343,960\n",
            "             GELU-13             [-1, 37, 3060]               0\n",
            "          Dropout-14             [-1, 37, 3060]               0\n",
            "           Linear-15              [-1, 37, 765]       2,341,665\n",
            "          Dropout-16              [-1, 37, 765]               0\n",
            "      ResidualAdd-17              [-1, 37, 765]               0\n",
            "        LayerNorm-18              [-1, 37, 765]           1,530\n",
            "           Linear-19             [-1, 37, 2295]       1,757,970\n",
            "          Dropout-20            [-1, 5, 37, 37]               0\n",
            "           Linear-21              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-22              [-1, 37, 765]               0\n",
            "          Dropout-23              [-1, 37, 765]               0\n",
            "      ResidualAdd-24              [-1, 37, 765]               0\n",
            "        LayerNorm-25              [-1, 37, 765]           1,530\n",
            "           Linear-26             [-1, 37, 3060]       2,343,960\n",
            "             GELU-27             [-1, 37, 3060]               0\n",
            "          Dropout-28             [-1, 37, 3060]               0\n",
            "           Linear-29              [-1, 37, 765]       2,341,665\n",
            "          Dropout-30              [-1, 37, 765]               0\n",
            "      ResidualAdd-31              [-1, 37, 765]               0\n",
            "        LayerNorm-32              [-1, 37, 765]           1,530\n",
            "           Linear-33             [-1, 37, 2295]       1,757,970\n",
            "          Dropout-34            [-1, 5, 37, 37]               0\n",
            "           Linear-35              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-36              [-1, 37, 765]               0\n",
            "          Dropout-37              [-1, 37, 765]               0\n",
            "      ResidualAdd-38              [-1, 37, 765]               0\n",
            "        LayerNorm-39              [-1, 37, 765]           1,530\n",
            "           Linear-40             [-1, 37, 3060]       2,343,960\n",
            "             GELU-41             [-1, 37, 3060]               0\n",
            "          Dropout-42             [-1, 37, 3060]               0\n",
            "           Linear-43              [-1, 37, 765]       2,341,665\n",
            "          Dropout-44              [-1, 37, 765]               0\n",
            "      ResidualAdd-45              [-1, 37, 765]               0\n",
            "        LayerNorm-46              [-1, 37, 765]           1,530\n",
            "           Linear-47             [-1, 37, 2295]       1,757,970\n",
            "          Dropout-48            [-1, 5, 37, 37]               0\n",
            "           Linear-49              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-50              [-1, 37, 765]               0\n",
            "          Dropout-51              [-1, 37, 765]               0\n",
            "      ResidualAdd-52              [-1, 37, 765]               0\n",
            "        LayerNorm-53              [-1, 37, 765]           1,530\n",
            "           Linear-54             [-1, 37, 3060]       2,343,960\n",
            "             GELU-55             [-1, 37, 3060]               0\n",
            "          Dropout-56             [-1, 37, 3060]               0\n",
            "           Linear-57              [-1, 37, 765]       2,341,665\n",
            "          Dropout-58              [-1, 37, 765]               0\n",
            "      ResidualAdd-59              [-1, 37, 765]               0\n",
            "        LayerNorm-60              [-1, 37, 765]           1,530\n",
            "           Linear-61             [-1, 37, 2295]       1,757,970\n",
            "          Dropout-62            [-1, 5, 37, 37]               0\n",
            "           Linear-63              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-64              [-1, 37, 765]               0\n",
            "          Dropout-65              [-1, 37, 765]               0\n",
            "      ResidualAdd-66              [-1, 37, 765]               0\n",
            "        LayerNorm-67              [-1, 37, 765]           1,530\n",
            "           Linear-68             [-1, 37, 3060]       2,343,960\n",
            "             GELU-69             [-1, 37, 3060]               0\n",
            "          Dropout-70             [-1, 37, 3060]               0\n",
            "           Linear-71              [-1, 37, 765]       2,341,665\n",
            "          Dropout-72              [-1, 37, 765]               0\n",
            "      ResidualAdd-73              [-1, 37, 765]               0\n",
            "        LayerNorm-74              [-1, 37, 765]           1,530\n",
            "           Linear-75             [-1, 37, 2295]       1,757,970\n",
            "          Dropout-76            [-1, 5, 37, 37]               0\n",
            "           Linear-77              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-78              [-1, 37, 765]               0\n",
            "          Dropout-79              [-1, 37, 765]               0\n",
            "      ResidualAdd-80              [-1, 37, 765]               0\n",
            "        LayerNorm-81              [-1, 37, 765]           1,530\n",
            "           Linear-82             [-1, 37, 3060]       2,343,960\n",
            "             GELU-83             [-1, 37, 3060]               0\n",
            "          Dropout-84             [-1, 37, 3060]               0\n",
            "           Linear-85              [-1, 37, 765]       2,341,665\n",
            "          Dropout-86              [-1, 37, 765]               0\n",
            "      ResidualAdd-87              [-1, 37, 765]               0\n",
            "        LayerNorm-88              [-1, 37, 765]           1,530\n",
            "           Linear-89             [-1, 37, 2295]       1,757,970\n",
            "          Dropout-90            [-1, 5, 37, 37]               0\n",
            "           Linear-91              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-92              [-1, 37, 765]               0\n",
            "          Dropout-93              [-1, 37, 765]               0\n",
            "      ResidualAdd-94              [-1, 37, 765]               0\n",
            "        LayerNorm-95              [-1, 37, 765]           1,530\n",
            "           Linear-96             [-1, 37, 3060]       2,343,960\n",
            "             GELU-97             [-1, 37, 3060]               0\n",
            "          Dropout-98             [-1, 37, 3060]               0\n",
            "           Linear-99              [-1, 37, 765]       2,341,665\n",
            "         Dropout-100              [-1, 37, 765]               0\n",
            "     ResidualAdd-101              [-1, 37, 765]               0\n",
            "       LayerNorm-102              [-1, 37, 765]           1,530\n",
            "          Linear-103             [-1, 37, 2295]       1,757,970\n",
            "         Dropout-104            [-1, 5, 37, 37]               0\n",
            "          Linear-105              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-106              [-1, 37, 765]               0\n",
            "         Dropout-107              [-1, 37, 765]               0\n",
            "     ResidualAdd-108              [-1, 37, 765]               0\n",
            "       LayerNorm-109              [-1, 37, 765]           1,530\n",
            "          Linear-110             [-1, 37, 3060]       2,343,960\n",
            "            GELU-111             [-1, 37, 3060]               0\n",
            "         Dropout-112             [-1, 37, 3060]               0\n",
            "          Linear-113              [-1, 37, 765]       2,341,665\n",
            "         Dropout-114              [-1, 37, 765]               0\n",
            "     ResidualAdd-115              [-1, 37, 765]               0\n",
            "       LayerNorm-116              [-1, 37, 765]           1,530\n",
            "          Linear-117             [-1, 37, 2295]       1,757,970\n",
            "         Dropout-118            [-1, 5, 37, 37]               0\n",
            "          Linear-119              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-120              [-1, 37, 765]               0\n",
            "         Dropout-121              [-1, 37, 765]               0\n",
            "     ResidualAdd-122              [-1, 37, 765]               0\n",
            "       LayerNorm-123              [-1, 37, 765]           1,530\n",
            "          Linear-124             [-1, 37, 3060]       2,343,960\n",
            "            GELU-125             [-1, 37, 3060]               0\n",
            "         Dropout-126             [-1, 37, 3060]               0\n",
            "          Linear-127              [-1, 37, 765]       2,341,665\n",
            "         Dropout-128              [-1, 37, 765]               0\n",
            "     ResidualAdd-129              [-1, 37, 765]               0\n",
            "       LayerNorm-130              [-1, 37, 765]           1,530\n",
            "          Linear-131             [-1, 37, 2295]       1,757,970\n",
            "         Dropout-132            [-1, 5, 37, 37]               0\n",
            "          Linear-133              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-134              [-1, 37, 765]               0\n",
            "         Dropout-135              [-1, 37, 765]               0\n",
            "     ResidualAdd-136              [-1, 37, 765]               0\n",
            "       LayerNorm-137              [-1, 37, 765]           1,530\n",
            "          Linear-138             [-1, 37, 3060]       2,343,960\n",
            "            GELU-139             [-1, 37, 3060]               0\n",
            "         Dropout-140             [-1, 37, 3060]               0\n",
            "          Linear-141              [-1, 37, 765]       2,341,665\n",
            "         Dropout-142              [-1, 37, 765]               0\n",
            "     ResidualAdd-143              [-1, 37, 765]               0\n",
            "       LayerNorm-144              [-1, 37, 765]           1,530\n",
            "          Linear-145             [-1, 37, 2295]       1,757,970\n",
            "         Dropout-146            [-1, 5, 37, 37]               0\n",
            "          Linear-147              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-148              [-1, 37, 765]               0\n",
            "         Dropout-149              [-1, 37, 765]               0\n",
            "     ResidualAdd-150              [-1, 37, 765]               0\n",
            "       LayerNorm-151              [-1, 37, 765]           1,530\n",
            "          Linear-152             [-1, 37, 3060]       2,343,960\n",
            "            GELU-153             [-1, 37, 3060]               0\n",
            "         Dropout-154             [-1, 37, 3060]               0\n",
            "          Linear-155              [-1, 37, 765]       2,341,665\n",
            "         Dropout-156              [-1, 37, 765]               0\n",
            "     ResidualAdd-157              [-1, 37, 765]               0\n",
            "       LayerNorm-158              [-1, 37, 765]           1,530\n",
            "          Linear-159             [-1, 37, 2295]       1,757,970\n",
            "         Dropout-160            [-1, 5, 37, 37]               0\n",
            "          Linear-161              [-1, 37, 765]         585,990\n",
            "MultiHeadAttention-162              [-1, 37, 765]               0\n",
            "         Dropout-163              [-1, 37, 765]               0\n",
            "     ResidualAdd-164              [-1, 37, 765]               0\n",
            "       LayerNorm-165              [-1, 37, 765]           1,530\n",
            "          Linear-166             [-1, 37, 3060]       2,343,960\n",
            "            GELU-167             [-1, 37, 3060]               0\n",
            "         Dropout-168             [-1, 37, 3060]               0\n",
            "          Linear-169              [-1, 37, 765]       2,341,665\n",
            "         Dropout-170              [-1, 37, 765]               0\n",
            "     ResidualAdd-171              [-1, 37, 765]               0\n",
            "================================================================\n",
            "Total params: 84,736,755\n",
            "Trainable params: 84,736,755\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 63.46\n",
            "Params size (MB): 323.25\n",
            "Estimated Total Size (MB): 386.76\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#연습\n",
        "model = ViT()\n",
        "x = torch.randn(1, 2, 60, 135)\n",
        "\n",
        "res = model(x)\n",
        "res= res.view(res.size(0),-1)\n",
        "\n",
        "\n",
        "head = nn.Linear(28305,2)\n",
        "model(x).shape,res.shape,head(torch.flatten(model(x))),head(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nR3gidQiYfa",
        "outputId": "23615cb8-e24e-4c3c-f5b0-60227c298885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 37, 765]),\n",
              " torch.Size([1, 28305]),\n",
              " tensor([0.6121, 1.4884], grad_fn=<AddBackward0>),\n",
              " tensor([[0.6121, 1.4884]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzezgo5KSJ9y"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvcNhga0SJ9y"
      },
      "outputs": [],
      "source": [
        "############ HYPERPARAMETERS ##############\n",
        "BATCH_SIZE = 128 # original = 128\n",
        "GAMMA = 0.999 # original = 0.999\n",
        "EPS_START = 0.9 # original = 0.9\n",
        "EPS_END = 0.01 # original = 0.05\n",
        "EPS_DECAY = 3000 # original = 200\n",
        "TARGET_UPDATE = 50 # original = 10\n",
        "MEMORY_SIZE = 100000 # original = 10000\n",
        "END_SCORE = 200 # 200 for Cartpole-v0\n",
        "TRAINING_STOP = 142 # threshold for training stop\n",
        "N_EPISODES = 50000 # total episodes to be run\n",
        "LAST_EPISODES_NUM = 20 # number of episodes for stopping training\n",
        "FRAMES = 2 # state is the number of last frames: the more frames, \n",
        "# the more the state is detailed (still Markovian)\n",
        "RESIZE_PIXELS = 60 # Downsample image to this number of pixels\n",
        "\n",
        "# ---- CONVOLUTIONAL NEURAL NETWORK ----\n",
        "HIDDEN_LAYER_1 = 64\n",
        "HIDDEN_LAYER_2 = 64 \n",
        "HIDDEN_LAYER_3 = 32\n",
        "KERNEL_SIZE = 5 # original = 5\n",
        "STRIDE = 2 # original = 2\n",
        "# --------------------------------------\n",
        "\n",
        "GRAYSCALE = True # False is RGB\n",
        "LOAD_MODEL = False # If we want to load the model, Default= False\n",
        "USE_CUDA = True # If we want to use GPU (powerful one needed!)\n",
        "############################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q8vgju9SJ9z"
      },
      "outputs": [],
      "source": [
        "graph_name = 'cartpole_vision'\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available() and USE_CUDA) else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpamHkj0SJ9z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Settings for GRAYSCALE / RGB\n",
        "if GRAYSCALE == 0:\n",
        "    resize = T.Compose([T.ToPILImage(), \n",
        "                    T.Resize((RESIZE_PIXELS), interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "    \n",
        "    nn_inputs = 3*FRAMES  # number of channels for the nn\n",
        "else:\n",
        "    resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize((RESIZE_PIXELS), interpolation=Image.CUBIC),\n",
        "                    T.Grayscale(),\n",
        "                    T.ToTensor()])\n",
        "    nn_inputs =  FRAMES # number of channels for the nn\n",
        "\n",
        "                    \n",
        "stop_training = False \n",
        "\n",
        "env = gym.make(\"CartPole-v0\").unwrapped \n",
        "\n",
        "# Set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# If gpu is to be used\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQexBb7ISJ90"
      },
      "source": [
        "## Build Experience Replay Buffer and Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA-V13hLSJ91"
      },
      "outputs": [],
      "source": [
        "# Memory for Experience Replay\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None) # if we haven't reached full capacity, we append a new transition\n",
        "        self.memory[self.position] = Transition(*args)  \n",
        "        self.position = (self.position + 1) % self.capacity # e.g if the capacity is 100, and our position is now 101, we don't append to\n",
        "        # position 101 (impossible), but to position 1 (its remainder), overwriting old data\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size) \n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CxvjKiLSJ91"
      },
      "outputs": [],
      "source": [
        "# Build CNN\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(nn_inputs, HIDDEN_LAYER_1, kernel_size=KERNEL_SIZE, stride=STRIDE) \n",
        "        self.bn1 = nn.BatchNorm2d(HIDDEN_LAYER_1)\n",
        "        self.conv2 = nn.Conv2d(HIDDEN_LAYER_1, HIDDEN_LAYER_2, kernel_size=KERNEL_SIZE, stride=STRIDE)\n",
        "        self.bn2 = nn.BatchNorm2d(HIDDEN_LAYER_2)\n",
        "        self.conv3 = nn.Conv2d(HIDDEN_LAYER_2, HIDDEN_LAYER_3, kernel_size=KERNEL_SIZE, stride=STRIDE)\n",
        "        self.bn3 = nn.BatchNorm2d(HIDDEN_LAYER_3)\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(size, kernel_size = KERNEL_SIZE, stride = STRIDE):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        nn.Dropout()\n",
        "        \n",
        "        #print(\"인풋 사이즈\",linear_input_size)\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        y = self.head(x.view(x.size(0), -1))\n",
        "        #print(\"info\",x.shape,x.size(0),x.view(x.size(0), -1).shape,y.shape)\n",
        "        return self.head(x.view(x.size(0), -1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build vision-transformer 3 depth\n",
        "class DQN2(nn.Module):    \n",
        "    \n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN2, self).__init__()\n",
        "        self.model = ViT()\n",
        "        nn.Dropout()\n",
        "        ex = torch.randn(1, FRAMES, h, w)\n",
        "        linear_input_size = torch.flatten(model(ex)).shape[0]\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = self.model(x)\n",
        "      \n",
        "        \n",
        "        return self.head(x.view(x.size(0),-1))"
      ],
      "metadata": {
        "id": "VGeHieQYj3kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ugtm3HKSJ92"
      },
      "outputs": [],
      "source": [
        "# Cart location for centering image crop\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "# Cropping, downsampling (and Grayscaling) image\n",
        "def get_screen():\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    screen = np.array(env.render(mode='rgb_array')).transpose((2, 0, 1))\n",
        "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # Strip off the edges, so that we have a square image centered on a cart\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "hK3hjamjSJ92",
        "outputId": "cf1619e7-9723-476b-d516-2b88579af7be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "if GRAYSCALE == 0:\n",
        "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "            interpolation='none')\n",
        "else:\n",
        "    plt.imshow(get_screen().cpu().squeeze(0).permute(\n",
        "        1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-YvYmCcSJ94"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O64clKg5SJ94",
        "outputId": "8c0f6f13-6046-4d4b-dfb3-126b217f3073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Screen height:  60  | Width:  135\n"
          ]
        }
      ],
      "source": [
        "eps_threshold = 0.9 # original = 0.9\n",
        "\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "print(\"Screen height: \", screen_height,\" | Width: \", screen_width)\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN2(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN2(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "if LOAD_MODEL == True:\n",
        "    policy_net_checkpoint = torch.load('save_model/policy_net_best3.pt') # best 3 is the default best\n",
        "    target_net_checkpoint = torch.load('save_model/target_net_best3.pt')\n",
        "    policy_net.load_state_dict(policy_net_checkpoint)\n",
        "    target_net.load_state_dict(target_net_checkpoint)\n",
        "    policy_net.eval()\n",
        "    target_net.eval()\n",
        "    stop_training = True # if we want to load, then we don't train the network anymore\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "# Action selection , if stop training == True, only exploitation\n",
        "def select_action(state, stop_training):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    # print('Epsilon = ', eps_threshold, end='\\n')\n",
        "    if sample > eps_threshold or stop_training:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "        \n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igR-xzhxSJ95"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "def plot_durations(score):\n",
        "    fig, ax = plt.subplots(figsize=(16,8))\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    episode_number = len(durations_t) \n",
        "    ax.set_title('Training...')\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Duration')\n",
        "    dur = durations_t.numpy()\n",
        "    plt.style.use('seaborn') #Change/Remove This If you Want\n",
        "#     ax.plot(dur, label= 'Score')\n",
        "    ax.fill_between(np.linspace(1,episode_number,episode_number),\n",
        "                    dur - dur.std(), dur + dur.std(), alpha=0.2)\n",
        "\n",
        "    plt.hlines(195, 0, episode_number, colors='red', linestyles=':', label='Win Threshold')\n",
        "    \n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        last100_mean = means[episode_number -100].item()\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        ax.plot(means.numpy(), label= 'Last 100 mean')\n",
        "        print('Episode: ', episode_number, ' | Score: ', score, '| Last 100 mean = ', last100_mean)\n",
        "    ax.legend(loc='upper left')\n",
        "    fig.savefig('plots/' + graph_name)\n",
        "#     if is_ipython:\n",
        "#         display.clear_output(wait=True)\n",
        "#         display.display(plt.gcf())\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHyas0HGSJ95"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlUckqeISJ95"
      },
      "outputs": [],
      "source": [
        "# Training \n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    # torch.cat concatenates tensor sequence\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward).type(torch.FloatTensor).to(device)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    plt.figure(2)\n",
        "    wandb.log({'Loss:': loss})\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsMgWDILSJ96"
      },
      "outputs": [],
      "source": [
        "episodes_trajectories = []\n",
        "episodes_after_stop = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1ABpIUkSJ96"
      },
      "source": [
        "# Main Loop\n",
        "Adjust the number of runs to see the effects on multiple trainings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-tSNKKoSJ96",
        "outputId": "525bdba4-fcdf-45a8-f53b-1fb004043ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 194/50000 [03:36<15:25:49,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-272f044eb4a7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mLAST_EPISODES_NUM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mTRAINING_STOP\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstop_training\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-e6616d54190a>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# state value or 0 in case the state was final.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mnext_state_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mnext_state_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_final_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Compute the expected Q values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mexpected_state_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_state_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "runs = 5\n",
        "\n",
        "# MAIN LOOP\n",
        "stop_training = False\n",
        "for j in range(runs):\n",
        "    mean_last = deque([0] * LAST_EPISODES_NUM, LAST_EPISODES_NUM)\n",
        "    policy_net = DQN2(screen_height, screen_width, n_actions).to(device)\n",
        "    target_net = DQN2(screen_height, screen_width, n_actions).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.RMSprop(policy_net.parameters())\n",
        "    memory = ReplayMemory(MEMORY_SIZE)\n",
        "    \n",
        "    count_final = 0\n",
        "    \n",
        "    steps_done = 0\n",
        "    episode_durations = []\n",
        "    for i_episode in tqdm(range(N_EPISODES)):\n",
        "        # Initialize the environment and state\n",
        "        env.reset()\n",
        "        init_screen = get_screen()\n",
        "        screens = deque([init_screen] * FRAMES, FRAMES)\n",
        "        state = torch.cat(list(screens), dim=1)\n",
        "\n",
        "        for t in count():\n",
        "\n",
        "            # Select and perform an action\n",
        "            #print(state.shape)\n",
        "            action = select_action(state, stop_training)\n",
        "            #print(env.step(action.item()))\n",
        "            state_variables, _, done, truncated,_ = env.step(action.item())\n",
        "\n",
        "            # Observe new state\n",
        "            screens.append(get_screen())\n",
        "            next_state = torch.cat(list(screens), dim=1) if not done else None\n",
        "\n",
        "            # Reward modification for better stability\n",
        "            x, x_dot, theta, theta_dot = state_variables\n",
        "            r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "            r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
        "            reward = r1 + r2\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            if t >= END_SCORE-1:\n",
        "                reward = reward + 20\n",
        "                done = 1\n",
        "            else: \n",
        "                if done:\n",
        "                    reward = reward - 20 \n",
        "\n",
        "            # Store the transition in memory\n",
        "            memory.push(state, action, next_state, reward)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            if done:\n",
        "                episode_durations.append(t + 1)\n",
        "                mean_last.append(t + 1)\n",
        "                mean = 0\n",
        "                wandb.log({'Episode duration': t+1 , 'Episode number': i_episode})\n",
        "                for i in range(LAST_EPISODES_NUM):\n",
        "                    mean = mean_last[i] + mean\n",
        "                mean = mean/LAST_EPISODES_NUM\n",
        "                if mean < TRAINING_STOP and stop_training == False:\n",
        "                    optimize_model()\n",
        "                else:\n",
        "                    stop_training = True\n",
        "                break\n",
        "\n",
        "        # Update the target network, copying all weights and biases in DQN\n",
        "        if i_episode % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "        if stop_training == True:\n",
        "            count_final += 1\n",
        "            if count_final >= 100:\n",
        "                break\n",
        "\n",
        "            \n",
        "    print('Complete')\n",
        "    stop_training = False\n",
        "    episodes_trajectories.append(episode_durations)\n",
        "    \n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFpEUCMzSJ97"
      },
      "source": [
        "# Data processing for plotting\n",
        "Obtain data from the best runs (aka, the ones where there was no vanishing gradients etc. Adjusting the model may result in more stable training!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea8HWmv6SJ97"
      },
      "outputs": [],
      "source": [
        "# Cherry picking best runs\n",
        "best = []\n",
        "best.append(episodes_trajectories[0])\n",
        "best.append(episodes_trajectories[1])\n",
        "best.append(episodes_trajectories[2])\n",
        "best.append(episodes_trajectories[3])\n",
        "#best.append(episodes_trajectories[5])\n",
        "#best.append(episodes_trajectories[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyuwpOKOSJ97"
      },
      "outputs": [],
      "source": [
        "maximum = 0\n",
        "for i in range(len(best)):\n",
        "    maximum = max(len(best[i]), maximum)\n",
        "    \n",
        "# Fill the episodes to make them the same length\n",
        "for i in range(len(best)):\n",
        "    length = len(best[i])\n",
        "    for j in range(maximum - len(best[i])):\n",
        "        best[i].append(best[i][j+length-100])\n",
        "    best[i] = np.asarray(best[i])\n",
        "    \n",
        "best = np.asarray(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiqB0OpuSJ98"
      },
      "outputs": [],
      "source": [
        "# To numpy\n",
        "score_mean = np.zeros(maximum)\n",
        "score_std = np.zeros(maximum)\n",
        "last100_mean = np.zeros(maximum)\n",
        "print(best[:, max(0, -99):1].mean())\n",
        "for i in range(maximum):\n",
        "    score_mean[i]  = best[:, i].mean()\n",
        "    score_std[i] = best[:, i].std()\n",
        "    last100_mean[i] = best[:, max(0, i-50):min(maximum, i+50)].mean()\n",
        "print(len(last100_mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTVmKuaSJ98"
      },
      "source": [
        "# Plotting\n",
        "Plot behaviour of combined runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0prl0etSJ98"
      },
      "outputs": [],
      "source": [
        "t = np.arange(0, maximum, 1)\n",
        "\n",
        "# from scipy.interpolate import make_interp_spline # make smooth version\n",
        "# interpol = make_interp_spline(t, score_mean, k=3)  # type: BSpline\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "ax.fill_between(t, np.maximum(score_mean - score_std, 0),\n",
        "                np.minimum(score_mean + score_std, 200), color='b', alpha=0.2)\n",
        "# ax.legend(loc='upper right')\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Score')\n",
        "# ax.set_title('Inverted Pendulum Training Plot from Pixels')\n",
        "ax.plot(t, score_mean, label='Score Mean')\n",
        "ax.plot(t, last100_mean, color='purple', linestyle='dotted', label='Smoothed mean')\n",
        "ax.legend()\n",
        "fig.savefig('score.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}